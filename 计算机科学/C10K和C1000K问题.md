## C10K
C10K：连接服务器的客户端数量达到10000+，即使硬件性能足够，也无法提供正常服务；

## 问题历史
计算机是指数型发展的，这一点无论从协议还是操作系统的设计都能看到当初设计对未来的预估严重不足；
- 比如在设计Unix的PID时用的就是16位张数，导致最多能创建的进程数不会超过32767个；如果一个客户端连接请求分配一个进程，再加上计算机操作系统自身占用的进程，那么能维持并发服务的连接数，远远无法满足10K；
- 比如C10K 以前，Linux 中网络处理都用同步阻塞的方式，也就是每个请求都分配一个进程或者线程。请求数只有 100 个时ok，但绝对无法应付10K；

## 常规解决方式（意味着无法解决）
- 用追加机器的方式：属于线性增长，远不能应付现在丰富的互联网需求；且机器在低峰时做不到物尽其用，产生资源浪费
- 操作系统优化：虚拟内存，每个进程都占用内存空间，用虚拟内存把硬盘资源序列化给内存使用，但随之而来的就是性能大幅下降；

## 问题点：解决之道
虽然10K听起来很吓人，但10K的连接中，真正需要服务器计算的确很少，也就是大量的时间、进程都花在维持连接上，傻傻的等待计算结果后的socket传输；并不是物尽其用；

Web 服务器从根本上来说是“I/O 密集型”而不是“CPU 密集型”，处理能力的关键在于网络收发而不是 CPU 计算（这里暂时不考虑 HTTPS 的加解密），而网络 I/O 会因为各式各样的原因不得不等待，比如数据还没到达、对端没有响应、缓冲区满发不出去等等；

因此解决之道在于：异步、事件触发、非阻塞IO；

### I/O 模型优化
利用IO多路复用：
背景知识：两种IO事件通知方式：
- 水平触发：只要文件描述符可以非阻塞地执行 I/O ，就会触发通知。也就是说，应用程序可以随时检查文件描述符的状态，然后再根据状态，进行 I/O 操作。
- 边缘触发：只有在文件描述符的状态发生改变（也就是 I/O 请求达到）时，才发送一次通知。这时候，应用程序需要尽可能多地执行 I/O，直到无法继续读写，才可以停止。如果 I/O 没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了；

第一种：使用非阻塞 I/O 和水平触发通知，比如使用 select 或者 poll

- select 和 poll 需要从文件描述符列表中，【轮询】找出哪些可以执行 I/O ，然后进行真正的网络 I/O 读写，【轮询】意味着什么？意味着至少On的复杂度；显然不是我们追求的；
- 除此之外，应用程序每次调用 select 和 poll 时，还需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间中。这一来一回的内核空间与用户空间切换，也增加了处理成本。

第二种：非阻塞 I/O 和边缘触发通知，比如 [[epoll]]

- epoll 使用事件驱动的机制，只关注有 I/O 事件发生的文件描述符，不需要轮询扫描整个集合
- epoll 使用红黑树，在内核中管理文件描述符的集合，这样，就不需要应用程序在每次操作时都传入、传出这个集合

第三种，使用异步 I/O（Asynchronous I/O，简称为 AIO）

异步 I/O 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。而在 I/O 完成后，系统会用事件通知（比如信号或者回调函数）的方式，告诉应用程序。这时，应用程序才会去查询 I/O 操作的结果

### 工作模型优化
第一种，主进程 + 多个 worker 子进程，这也是最常用的一种模型
（nginx、node：）主进程主要用来初始化套接字，并管理子进程的生命周期；而 worker 进程，则负责实际的请求处理

第二种，监听到相同端口的多进程模型
所有的进程都监听相同的接口，并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中去


## C1000K 是不是也可以用上述方案解决？
总结：C10K：用I/O 多路复用，“大名鼎鼎”的 epoll得到解决；

从 C10K 到 C100K ，可能只需要增加系统的物理资源就可以满足；但从 C100K 到 C1000K ，就不仅仅是增加物理资源就能解决的问题了；

需要多方面的优化工作了，从硬件的中断处理和网络功能卸载、到网络协议栈的文件描述符数量、连接状态跟踪、缓存队列等内核的优化，再到应用程序的工作模型优化

C10M ，就不只是增加物理资源，或者优化内核和应用程序可以解决的问题了。这时候，就需要用：
- XDP 的方式，在内核协议栈之前处理网络包；
- DPDK 直接跳过网络协议栈，在用户空间通过轮询的方式直接处理网络包
上述方案离前端太远，暂不做进一步分析；